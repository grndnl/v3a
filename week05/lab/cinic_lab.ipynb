{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cace261f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d693cd6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEED=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "363e2b75",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d479743",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "START_EPOCH = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec40e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set the architecture to resnet 18 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97b1a0aa",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "ARCH = 'resnet18'# set the architecture to RESNET 18\n",
    "# please look up how to do that\n",
    "########################\n",
    "EPOCHS = 5\n",
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "PRINT_FREQ = 50\n",
    "TRAIN_BATCH=128\n",
    "VAL_BATCH=128\n",
    "WORKERS=2\n",
    "# TRAINDIR=\"/workspace/data/imagenet2012/train\"\n",
    "# VALDIR=\"/workspace/data/imagenet2012/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09de23aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TRAINDIR=\"/CINIC/train\"\n",
    "# VALDIR=\"/CINIC/valid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c3188",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Check if cuda is available here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a0499b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cuda is available in this cell\n",
    "# if it is not available, you should not go forward!\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c35f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assign your GPU below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252a08a3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Assign your GPU in this cell\n",
    "GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e7ec23",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set your active device to your GPU in this cell\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e5ddf20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# enable algorithm optimization\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdd6e12",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fill in the heart of the train section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70dbd4c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    data_time = AverageMeter('Data', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [batch_time, data_time, losses, top1, top5],\n",
    "        prefix=\"Epoch: [{}]\".format(epoch))\n",
    "\n",
    "    ######################\n",
    "    # switch model to train mode here\n",
    "    model.train()\n",
    "    ################\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        #####################\n",
    "        # send the images to cuda device\n",
    "        # send the target to cuda device\n",
    "        images = torch.tensor(images).to('cuda')\n",
    "        target = torch.tensor(target).to('cuda')\n",
    "\n",
    "\n",
    "        # compute output\n",
    "        # output = model ?? images\n",
    "        output = model(images)\n",
    "\n",
    "        # compute loss \n",
    "        # loss = criterion, output, target\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        \n",
    "        #### zero out gradients in the optimier\n",
    "        ## optimizer ..??\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## backprop!\n",
    "        ### loss... ???\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights!\n",
    "        ## optimier .. ??\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % PRINT_FREQ == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb3b62e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Fill in the validate section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3f276cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    # model ???\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            ### send the images and target to cuda\n",
    "            images = torch.tensor(images).to('cuda')\n",
    "            target = torch.tensor(target).to('cuda')\n",
    "\n",
    "            # compute output\n",
    "            # output = model ??? images?\n",
    "            output = model(images)\n",
    "\n",
    "            # compute loss\n",
    "            # loss  = criterion ?? output ?? target\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % PRINT_FREQ == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe49226",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Save the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff8a4159",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    # save the model state!\n",
    "    # state ???\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cd7ea3a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1da87ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00211030",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if we are adjusting the LR manually use this\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = LR * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da2c1382",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c29e7a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imagenet_mean_RGB = [0.47889522, 0.47227842, 0.43047404]\n",
    "imagenet_std_RGB = [0.229, 0.224, 0.225]\n",
    "cinic_mean_RGB = [0.47889522, 0.47227842, 0.43047404]\n",
    "cinic_std_RGB = [0.24205776, 0.23828046, 0.25874835]\n",
    "cifar_mean_RGB = [0.4914, 0.4822, 0.4465]\n",
    "cifar_std_RGB = [0.2023, 0.1994, 0.2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1a61c02",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=cifar_mean_RGB, std=cifar_std_RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47dd3e49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "# IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4387cb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initialize the model using the architecture you selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1abcc33",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select the model\n",
    "# model = ...\n",
    "model = models.get_model(ARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.fc = nn.Linear(512, 10)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "2db1bb69",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Send the model to the cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d23ccb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# send the model to the cuda device..\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8eb8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Instantiate the loss to cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a49ae3c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a49045a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Instantiate the optimizer to SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa3a04dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use SGD .. use the momentum and weight decay vars\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ef11d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create the learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0e1727a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use CosineAnnealingLR\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fe08caa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_mean_RGB, cifar_std_RGB),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236528e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create the train dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c29f6b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use torchvision.datasets.CIFAR10\n",
    "train_dataset = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder('data/cinic/train',\n",
    "    \ttransform=transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cinic_mean_RGB,std=cinic_std_RGB)])),\n",
    "    batch_size=TRAIN_BATCH, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63dfe3c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar_mean_RGB, cifar_std_RGB),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca6c39",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create the val dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42d58f82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use torchvision.datasets.CIFAR10\n",
    "val_dataset = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.ImageFolder('data/cinic/valid',\n",
    "    \ttransform=transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cinic_mean_RGB,std=cinic_std_RGB)])),\n",
    "    batch_size=VAL_BATCH, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a291660",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create the train dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "574373be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fill this in\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset.dataset, batch_size=TRAIN_BATCH, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280c6e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create the val dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6aa623fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fill this in..\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset.dataset, batch_size=VAL_BATCH, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cfa6766",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_acc1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d0620a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grandid\\AppData\\Local\\Temp\\1\\ipykernel_18552\\2208984069.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images = torch.tensor(images).to('cuda')\n",
      "C:\\Users\\grandid\\AppData\\Local\\Temp\\1\\ipykernel_18552\\2208984069.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to('cuda')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][  0/704]\tTime  4.865 ( 4.865)\tData  0.134 ( 0.134)\tLoss 2.6469e+00 (2.6469e+00)\tAcc@1   8.59 (  8.59)\tAcc@5  38.28 ( 38.28)\n",
      "Epoch: [0][ 50/704]\tTime  0.100 ( 0.193)\tData  0.083 ( 0.085)\tLoss 2.2938e+00 (3.4218e+00)\tAcc@1  13.28 ( 18.90)\tAcc@5  67.97 ( 66.73)\n",
      "Epoch: [0][100/704]\tTime  0.103 ( 0.148)\tData  0.085 ( 0.085)\tLoss 2.0075e+00 (2.9091e+00)\tAcc@1  26.56 ( 20.86)\tAcc@5  85.16 ( 71.48)\n",
      "Epoch: [0][150/704]\tTime  0.099 ( 0.133)\tData  0.083 ( 0.085)\tLoss 1.9249e+00 (2.6103e+00)\tAcc@1  24.22 ( 23.34)\tAcc@5  83.59 ( 75.41)\n",
      "Epoch: [0][200/704]\tTime  0.102 ( 0.125)\tData  0.084 ( 0.084)\tLoss 1.7742e+00 (2.4397e+00)\tAcc@1  33.59 ( 24.90)\tAcc@5  86.72 ( 77.48)\n",
      "Epoch: [0][250/704]\tTime  0.102 ( 0.120)\tData  0.084 ( 0.084)\tLoss 1.8089e+00 (2.3185e+00)\tAcc@1  32.03 ( 26.23)\tAcc@5  84.38 ( 79.21)\n",
      "Epoch: [0][300/704]\tTime  0.102 ( 0.117)\tData  0.084 ( 0.084)\tLoss 1.6336e+00 (2.2292e+00)\tAcc@1  43.75 ( 27.31)\tAcc@5  91.41 ( 80.45)\n",
      "Epoch: [0][350/704]\tTime  0.101 ( 0.115)\tData  0.083 ( 0.084)\tLoss 1.8684e+00 (2.1593e+00)\tAcc@1  34.38 ( 28.49)\tAcc@5  82.03 ( 81.42)\n",
      "Epoch: [0][400/704]\tTime  0.102 ( 0.113)\tData  0.084 ( 0.084)\tLoss 1.7082e+00 (2.1079e+00)\tAcc@1  34.38 ( 29.22)\tAcc@5  89.06 ( 82.14)\n",
      "Epoch: [0][450/704]\tTime  0.101 ( 0.112)\tData  0.083 ( 0.084)\tLoss 1.6294e+00 (2.0621e+00)\tAcc@1  41.41 ( 30.12)\tAcc@5  92.19 ( 82.83)\n",
      "Epoch: [0][500/704]\tTime  0.101 ( 0.111)\tData  0.083 ( 0.084)\tLoss 1.6190e+00 (2.0251e+00)\tAcc@1  40.62 ( 30.82)\tAcc@5  85.16 ( 83.37)\n",
      "Epoch: [0][550/704]\tTime  0.101 ( 0.110)\tData  0.084 ( 0.084)\tLoss 1.7247e+00 (1.9917e+00)\tAcc@1  35.16 ( 31.52)\tAcc@5  88.28 ( 83.93)\n",
      "Epoch: [0][600/704]\tTime  0.101 ( 0.110)\tData  0.084 ( 0.084)\tLoss 1.6227e+00 (1.9622e+00)\tAcc@1  39.06 ( 32.12)\tAcc@5  90.62 ( 84.40)\n",
      "Epoch: [0][650/704]\tTime  0.100 ( 0.109)\tData  0.083 ( 0.084)\tLoss 1.4437e+00 (1.9371e+00)\tAcc@1  51.56 ( 32.72)\tAcc@5  92.19 ( 84.72)\n",
      "Epoch: [0][700/704]\tTime  0.101 ( 0.108)\tData  0.083 ( 0.084)\tLoss 1.4584e+00 (1.9128e+00)\tAcc@1  47.66 ( 33.31)\tAcc@5  92.97 ( 85.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grandid\\AppData\\Local\\Temp\\1\\ipykernel_18552\\3161145004.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  images = torch.tensor(images).to('cuda')\n",
      "C:\\Users\\grandid\\AppData\\Local\\Temp\\1\\ipykernel_18552\\3161145004.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target).to('cuda')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [  0/704]\tTime  0.103 ( 0.103)\tLoss 1.3248e+00 (1.3248e+00)\tAcc@1  60.16 ( 60.16)\tAcc@5  84.38 ( 84.38)\n",
      "Test: [ 50/704]\tTime  0.111 ( 0.097)\tLoss 8.0303e-01 (1.1987e+00)\tAcc@1  75.78 ( 62.68)\tAcc@5  95.31 ( 87.06)\n",
      "Test: [100/704]\tTime  0.110 ( 0.103)\tLoss 1.4020e+00 (1.2997e+00)\tAcc@1  50.00 ( 56.88)\tAcc@5  93.75 ( 88.51)\n",
      "Test: [150/704]\tTime  0.110 ( 0.105)\tLoss 2.2977e+00 (1.4760e+00)\tAcc@1  13.28 ( 49.37)\tAcc@5  77.34 ( 87.29)\n",
      "Test: [200/704]\tTime  0.109 ( 0.106)\tLoss 2.3501e+00 (1.7013e+00)\tAcc@1   4.69 ( 39.66)\tAcc@5  76.56 ( 84.17)\n",
      "Test: [250/704]\tTime  0.110 ( 0.107)\tLoss 1.5723e+00 (1.7130e+00)\tAcc@1  21.88 ( 35.26)\tAcc@5  96.09 ( 85.61)\n",
      "Test: [300/704]\tTime  0.109 ( 0.107)\tLoss 2.0671e+00 (1.7227e+00)\tAcc@1  16.41 ( 32.85)\tAcc@5  93.75 ( 87.20)\n",
      "Test: [350/704]\tTime  0.109 ( 0.107)\tLoss 2.1988e+00 (1.7623e+00)\tAcc@1  13.28 ( 31.20)\tAcc@5  91.41 ( 88.11)\n",
      "Test: [400/704]\tTime  0.110 ( 0.108)\tLoss 1.3109e+00 (1.7210e+00)\tAcc@1  60.94 ( 33.92)\tAcc@5  97.66 ( 89.03)\n",
      "Test: [450/704]\tTime  0.107 ( 0.108)\tLoss 1.3403e+00 (1.6983e+00)\tAcc@1  63.28 ( 35.59)\tAcc@5  94.53 ( 89.53)\n",
      "Test: [500/704]\tTime  0.108 ( 0.108)\tLoss 1.3866e+00 (1.6669e+00)\tAcc@1  53.12 ( 37.80)\tAcc@5  91.41 ( 89.86)\n",
      "Test: [550/704]\tTime  0.110 ( 0.108)\tLoss 1.4087e+00 (1.6445e+00)\tAcc@1  51.56 ( 39.13)\tAcc@5  90.62 ( 89.87)\n",
      "Test: [600/704]\tTime  0.109 ( 0.108)\tLoss 2.1379e+00 (1.6404e+00)\tAcc@1  28.91 ( 39.39)\tAcc@5  68.75 ( 89.46)\n",
      "Test: [650/704]\tTime  0.112 ( 0.108)\tLoss 1.6562e+00 (1.6367e+00)\tAcc@1  35.94 ( 39.58)\tAcc@5  90.62 ( 89.27)\n",
      "Test: [700/704]\tTime  0.110 ( 0.108)\tLoss 1.4856e+00 (1.6362e+00)\tAcc@1  38.28 ( 39.41)\tAcc@5  94.53 ( 89.38)\n",
      " * Acc@1 39.417 Acc@5 89.393\n",
      "lr: [0.0993844170297569]\n",
      "Epoch: [1][  0/704]\tTime  0.128 ( 0.128)\tData  0.094 ( 0.094)\tLoss 1.6260e+00 (1.6260e+00)\tAcc@1  39.06 ( 39.06)\tAcc@5  89.06 ( 89.06)\n",
      "Epoch: [1][ 50/704]\tTime  0.101 ( 0.104)\tData  0.084 ( 0.086)\tLoss 1.6391e+00 (1.5855e+00)\tAcc@1  35.94 ( 41.27)\tAcc@5  88.28 ( 90.29)\n",
      "Epoch: [1][100/704]\tTime  0.100 ( 0.103)\tData  0.082 ( 0.085)\tLoss 1.5423e+00 (1.5655e+00)\tAcc@1  37.50 ( 41.91)\tAcc@5  91.41 ( 90.62)\n",
      "Epoch: [1][150/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.085)\tLoss 1.5514e+00 (1.5511e+00)\tAcc@1  44.53 ( 42.45)\tAcc@5  89.84 ( 90.95)\n",
      "Epoch: [1][200/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.4901e+00 (1.5385e+00)\tAcc@1  46.88 ( 43.26)\tAcc@5  90.62 ( 91.01)\n",
      "Epoch: [1][250/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.3872e+00 (1.5261e+00)\tAcc@1  50.00 ( 43.69)\tAcc@5  95.31 ( 91.26)\n",
      "Epoch: [1][300/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.5665e+00 (1.5157e+00)\tAcc@1  46.88 ( 44.10)\tAcc@5  85.94 ( 91.38)\n",
      "Epoch: [1][350/704]\tTime  0.100 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.3928e+00 (1.5049e+00)\tAcc@1  48.44 ( 44.60)\tAcc@5  92.19 ( 91.49)\n",
      "Epoch: [1][400/704]\tTime  0.102 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.3289e+00 (1.4947e+00)\tAcc@1  45.31 ( 45.07)\tAcc@5  93.75 ( 91.67)\n",
      "Epoch: [1][450/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.4668e+00 (1.4870e+00)\tAcc@1  45.31 ( 45.46)\tAcc@5  92.97 ( 91.69)\n",
      "Epoch: [1][500/704]\tTime  0.100 ( 0.102)\tData  0.082 ( 0.084)\tLoss 1.4090e+00 (1.4839e+00)\tAcc@1  50.78 ( 45.58)\tAcc@5  97.66 ( 91.74)\n",
      "Epoch: [1][550/704]\tTime  0.102 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.4457e+00 (1.4761e+00)\tAcc@1  49.22 ( 45.89)\tAcc@5  93.75 ( 91.88)\n",
      "Epoch: [1][600/704]\tTime  0.103 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.3478e+00 (1.4708e+00)\tAcc@1  45.31 ( 46.07)\tAcc@5  96.88 ( 91.93)\n",
      "Epoch: [1][650/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.4901e+00 (1.4672e+00)\tAcc@1  47.66 ( 46.23)\tAcc@5  89.84 ( 91.98)\n",
      "Epoch: [1][700/704]\tTime  0.102 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.3292e+00 (1.4624e+00)\tAcc@1  52.34 ( 46.44)\tAcc@5  96.09 ( 92.08)\n",
      "Test: [  0/704]\tTime  0.093 ( 0.093)\tLoss 9.6436e-01 (9.6436e-01)\tAcc@1  69.53 ( 69.53)\tAcc@5  92.19 ( 92.19)\n",
      "Test: [ 50/704]\tTime  0.111 ( 0.103)\tLoss 7.7508e-01 (9.1329e-01)\tAcc@1  70.31 ( 68.41)\tAcc@5  96.09 ( 94.52)\n",
      "Test: [100/704]\tTime  0.110 ( 0.106)\tLoss 1.1649e+00 (1.0474e+00)\tAcc@1  40.62 ( 57.80)\tAcc@5  97.66 ( 94.91)\n",
      "Test: [150/704]\tTime  0.109 ( 0.108)\tLoss 1.5509e+00 (1.2139e+00)\tAcc@1  39.06 ( 48.99)\tAcc@5  92.97 ( 94.13)\n",
      "Test: [200/704]\tTime  0.115 ( 0.109)\tLoss 1.4696e+00 (1.3342e+00)\tAcc@1  43.75 ( 45.60)\tAcc@5  95.31 ( 94.00)\n",
      "Test: [250/704]\tTime  0.110 ( 0.109)\tLoss 1.7126e+00 (1.4070e+00)\tAcc@1  37.50 ( 43.66)\tAcc@5  85.94 ( 93.23)\n",
      "Test: [300/704]\tTime  0.110 ( 0.109)\tLoss 1.7339e+00 (1.4307e+00)\tAcc@1  38.28 ( 43.76)\tAcc@5  89.06 ( 93.27)\n",
      "Test: [350/704]\tTime  0.109 ( 0.109)\tLoss 1.7461e+00 (1.4382e+00)\tAcc@1  26.56 ( 44.32)\tAcc@5  90.62 ( 93.34)\n",
      "Test: [400/704]\tTime  0.109 ( 0.109)\tLoss 1.4034e+00 (1.4553e+00)\tAcc@1  39.06 ( 43.33)\tAcc@5  95.31 ( 93.66)\n",
      "Test: [450/704]\tTime  0.109 ( 0.109)\tLoss 1.3252e+00 (1.4776e+00)\tAcc@1  57.81 ( 42.81)\tAcc@5  90.62 ( 93.41)\n",
      "Test: [500/704]\tTime  0.109 ( 0.109)\tLoss 1.5127e+00 (1.4783e+00)\tAcc@1  51.56 ( 43.76)\tAcc@5  86.72 ( 92.96)\n",
      "Test: [550/704]\tTime  0.109 ( 0.109)\tLoss 1.7813e+00 (1.4808e+00)\tAcc@1  46.09 ( 44.42)\tAcc@5  79.69 ( 92.23)\n",
      "Test: [600/704]\tTime  0.110 ( 0.109)\tLoss 1.4940e+00 (1.4588e+00)\tAcc@1  52.34 ( 45.65)\tAcc@5  90.62 ( 92.33)\n",
      "Test: [650/704]\tTime  0.110 ( 0.110)\tLoss 1.2590e+00 (1.4303e+00)\tAcc@1  62.50 ( 47.06)\tAcc@5  91.41 ( 92.53)\n",
      "Test: [700/704]\tTime  0.109 ( 0.110)\tLoss 9.7837e-01 (1.4075e+00)\tAcc@1  67.97 ( 48.34)\tAcc@5  99.22 ( 92.59)\n",
      " * Acc@1 48.404 Acc@5 92.602\n",
      "lr: [0.09755282581475769]\n",
      "Epoch: [2][  0/704]\tTime  0.123 ( 0.123)\tData  0.089 ( 0.089)\tLoss 1.1596e+00 (1.1596e+00)\tAcc@1  58.59 ( 58.59)\tAcc@5  93.75 ( 93.75)\n",
      "Epoch: [2][ 50/704]\tTime  0.100 ( 0.103)\tData  0.083 ( 0.086)\tLoss 1.3317e+00 (1.4343e+00)\tAcc@1  51.56 ( 48.19)\tAcc@5  96.09 ( 92.54)\n",
      "Epoch: [2][100/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.085)\tLoss 1.2061e+00 (1.4014e+00)\tAcc@1  54.69 ( 49.03)\tAcc@5  96.88 ( 92.84)\n",
      "Epoch: [2][150/704]\tTime  0.101 ( 0.103)\tData  0.083 ( 0.085)\tLoss 1.2501e+00 (1.3862e+00)\tAcc@1  53.91 ( 49.37)\tAcc@5  95.31 ( 93.24)\n",
      "Epoch: [2][200/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.085)\tLoss 1.2859e+00 (1.3733e+00)\tAcc@1  53.91 ( 49.87)\tAcc@5  96.09 ( 93.29)\n",
      "Epoch: [2][250/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.4356e+00 (1.3643e+00)\tAcc@1  48.44 ( 50.23)\tAcc@5  92.97 ( 93.29)\n",
      "Epoch: [2][300/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.2401e+00 (1.3632e+00)\tAcc@1  52.34 ( 50.25)\tAcc@5  95.31 ( 93.28)\n",
      "Epoch: [2][350/704]\tTime  0.102 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.2983e+00 (1.3552e+00)\tAcc@1  48.44 ( 50.58)\tAcc@5  92.97 ( 93.32)\n",
      "Epoch: [2][400/704]\tTime  0.102 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.4915e+00 (1.3521e+00)\tAcc@1  46.09 ( 50.83)\tAcc@5  92.97 ( 93.34)\n",
      "Epoch: [2][450/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.3392e+00 (1.3485e+00)\tAcc@1  50.78 ( 50.95)\tAcc@5  94.53 ( 93.34)\n",
      "Epoch: [2][500/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.4730e+00 (1.3455e+00)\tAcc@1  46.09 ( 51.06)\tAcc@5  91.41 ( 93.39)\n",
      "Epoch: [2][550/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.2328e+00 (1.3398e+00)\tAcc@1  60.16 ( 51.22)\tAcc@5  91.41 ( 93.48)\n",
      "Epoch: [2][600/704]\tTime  0.101 ( 0.102)\tData  0.082 ( 0.084)\tLoss 1.1914e+00 (1.3350e+00)\tAcc@1  55.47 ( 51.40)\tAcc@5  97.66 ( 93.57)\n",
      "Epoch: [2][650/704]\tTime  0.100 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.3168e+00 (1.3321e+00)\tAcc@1  50.78 ( 51.53)\tAcc@5  96.88 ( 93.60)\n",
      "Epoch: [2][700/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.1825e+00 (1.3278e+00)\tAcc@1  56.25 ( 51.74)\tAcc@5  95.31 ( 93.66)\n",
      "Test: [  0/704]\tTime  0.094 ( 0.094)\tLoss 1.0193e+00 (1.0193e+00)\tAcc@1  64.06 ( 64.06)\tAcc@5  93.75 ( 93.75)\n",
      "Test: [ 50/704]\tTime  0.110 ( 0.105)\tLoss 1.0101e+00 (1.0619e+00)\tAcc@1  61.72 ( 62.12)\tAcc@5  92.19 ( 92.88)\n",
      "Test: [100/704]\tTime  0.110 ( 0.107)\tLoss 7.9558e-01 (1.0665e+00)\tAcc@1  73.44 ( 62.05)\tAcc@5  97.66 ( 93.28)\n",
      "Test: [150/704]\tTime  0.111 ( 0.108)\tLoss 1.3036e+00 (1.1446e+00)\tAcc@1  53.12 ( 59.54)\tAcc@5  95.31 ( 92.62)\n",
      "Test: [200/704]\tTime  0.110 ( 0.109)\tLoss 1.2114e+00 (1.2124e+00)\tAcc@1  57.03 ( 57.02)\tAcc@5  98.44 ( 93.71)\n",
      "Test: [250/704]\tTime  0.109 ( 0.109)\tLoss 1.4353e+00 (1.2685e+00)\tAcc@1  39.84 ( 52.90)\tAcc@5  96.88 ( 94.38)\n",
      "Test: [300/704]\tTime  0.111 ( 0.109)\tLoss 1.7282e+00 (1.3195e+00)\tAcc@1  31.25 ( 50.11)\tAcc@5  97.66 ( 94.97)\n",
      "Test: [350/704]\tTime  0.109 ( 0.109)\tLoss 1.5246e+00 (1.3519e+00)\tAcc@1  38.28 ( 48.94)\tAcc@5  98.44 ( 95.34)\n",
      "Test: [400/704]\tTime  0.111 ( 0.109)\tLoss 1.1834e+00 (1.3552e+00)\tAcc@1  53.12 ( 48.60)\tAcc@5  97.66 ( 95.59)\n",
      "Test: [450/704]\tTime  0.110 ( 0.109)\tLoss 6.4585e-01 (1.3459e+00)\tAcc@1  82.03 ( 48.91)\tAcc@5  96.88 ( 95.62)\n",
      "Test: [500/704]\tTime  0.110 ( 0.109)\tLoss 9.4319e-01 (1.3025e+00)\tAcc@1  69.53 ( 51.05)\tAcc@5  94.53 ( 95.56)\n",
      "Test: [550/704]\tTime  0.109 ( 0.109)\tLoss 1.2999e+00 (1.2783e+00)\tAcc@1  53.12 ( 52.47)\tAcc@5  89.84 ( 95.36)\n",
      "Test: [600/704]\tTime  0.110 ( 0.109)\tLoss 1.5324e+00 (1.2620e+00)\tAcc@1  51.56 ( 53.76)\tAcc@5  82.81 ( 94.96)\n",
      "Test: [650/704]\tTime  0.109 ( 0.109)\tLoss 1.3669e+00 (1.2596e+00)\tAcc@1  61.72 ( 54.38)\tAcc@5  85.94 ( 94.52)\n",
      "Test: [700/704]\tTime  0.109 ( 0.109)\tLoss 1.2826e+00 (1.2781e+00)\tAcc@1  45.31 ( 53.65)\tAcc@5  92.19 ( 93.96)\n",
      " * Acc@1 53.626 Acc@5 93.956\n",
      "lr: [0.0945503262094184]\n",
      "Epoch: [3][  0/704]\tTime  0.123 ( 0.123)\tData  0.089 ( 0.089)\tLoss 1.0963e+00 (1.0963e+00)\tAcc@1  60.94 ( 60.94)\tAcc@5  96.88 ( 96.88)\n",
      "Epoch: [3][ 50/704]\tTime  0.100 ( 0.103)\tData  0.082 ( 0.086)\tLoss 1.2282e+00 (1.2773e+00)\tAcc@1  56.25 ( 54.66)\tAcc@5  92.19 ( 94.12)\n",
      "Epoch: [3][100/704]\tTime  0.102 ( 0.103)\tData  0.084 ( 0.085)\tLoss 1.0751e+00 (1.2493e+00)\tAcc@1  62.50 ( 55.31)\tAcc@5  95.31 ( 94.40)\n",
      "Epoch: [3][150/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.085)\tLoss 1.4408e+00 (1.2447e+00)\tAcc@1  48.44 ( 55.37)\tAcc@5  92.19 ( 94.46)\n",
      "Epoch: [3][200/704]\tTime  0.103 ( 0.102)\tData  0.084 ( 0.085)\tLoss 1.0572e+00 (1.2407e+00)\tAcc@1  60.16 ( 55.45)\tAcc@5  96.88 ( 94.41)\n",
      "Epoch: [3][250/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.1393e+00 (1.2368e+00)\tAcc@1  61.72 ( 55.68)\tAcc@5  96.09 ( 94.47)\n",
      "Epoch: [3][300/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.2415e+00 (1.2329e+00)\tAcc@1  55.47 ( 55.80)\tAcc@5  96.09 ( 94.58)\n",
      "Epoch: [3][350/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.1735e+00 (1.2308e+00)\tAcc@1  64.06 ( 55.84)\tAcc@5  94.53 ( 94.58)\n",
      "Epoch: [3][400/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.2986e+00 (1.2296e+00)\tAcc@1  52.34 ( 55.87)\tAcc@5  93.75 ( 94.58)\n",
      "Epoch: [3][450/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.1834e+00 (1.2299e+00)\tAcc@1  55.47 ( 55.80)\tAcc@5  97.66 ( 94.62)\n",
      "Epoch: [3][500/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.1236e+00 (1.2286e+00)\tAcc@1  62.50 ( 55.85)\tAcc@5  96.88 ( 94.63)\n",
      "Epoch: [3][550/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.3401e+00 (1.2257e+00)\tAcc@1  56.25 ( 55.98)\tAcc@5  92.19 ( 94.67)\n",
      "Epoch: [3][600/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.2053e+00 (1.2255e+00)\tAcc@1  54.69 ( 55.97)\tAcc@5  96.09 ( 94.69)\n",
      "Epoch: [3][650/704]\tTime  0.101 ( 0.102)\tData  0.082 ( 0.084)\tLoss 9.9509e-01 (1.2245e+00)\tAcc@1  62.50 ( 55.95)\tAcc@5  96.88 ( 94.72)\n",
      "Epoch: [3][700/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.0457e+00 (1.2241e+00)\tAcc@1  62.50 ( 56.03)\tAcc@5  97.66 ( 94.73)\n",
      "Test: [  0/704]\tTime  0.094 ( 0.094)\tLoss 1.1715e+00 (1.1715e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  92.97 ( 92.97)\n",
      "Test: [ 50/704]\tTime  0.122 ( 0.105)\tLoss 9.4675e-01 (1.0940e+00)\tAcc@1  67.97 ( 62.04)\tAcc@5  91.41 ( 92.26)\n",
      "Test: [100/704]\tTime  0.109 ( 0.107)\tLoss 8.0880e-01 (1.1156e+00)\tAcc@1  65.62 ( 60.43)\tAcc@5  96.88 ( 92.98)\n",
      "Test: [150/704]\tTime  0.109 ( 0.108)\tLoss 1.1493e+00 (1.1850e+00)\tAcc@1  55.47 ( 57.32)\tAcc@5  99.22 ( 92.91)\n",
      "Test: [200/704]\tTime  0.111 ( 0.108)\tLoss 1.0422e+00 (1.1961e+00)\tAcc@1  65.62 ( 56.84)\tAcc@5 100.00 ( 94.18)\n",
      "Test: [250/704]\tTime  0.109 ( 0.108)\tLoss 1.4839e+00 (1.2393e+00)\tAcc@1  50.00 ( 55.31)\tAcc@5  98.44 ( 94.79)\n",
      "Test: [300/704]\tTime  0.112 ( 0.109)\tLoss 1.5008e+00 (1.2751e+00)\tAcc@1  42.97 ( 53.91)\tAcc@5  98.44 ( 95.29)\n",
      "Test: [350/704]\tTime  0.110 ( 0.109)\tLoss 1.5996e+00 (1.3094e+00)\tAcc@1  37.50 ( 52.68)\tAcc@5  98.44 ( 95.39)\n",
      "Test: [400/704]\tTime  0.110 ( 0.109)\tLoss 1.3794e+00 (1.3272e+00)\tAcc@1  38.28 ( 51.07)\tAcc@5  99.22 ( 95.68)\n",
      "Test: [450/704]\tTime  0.109 ( 0.109)\tLoss 6.9646e-01 (1.3065e+00)\tAcc@1  80.47 ( 51.52)\tAcc@5  96.09 ( 95.81)\n",
      "Test: [500/704]\tTime  0.110 ( 0.109)\tLoss 1.1862e+00 (1.2569e+00)\tAcc@1  57.03 ( 53.78)\tAcc@5  92.19 ( 95.81)\n",
      "Test: [550/704]\tTime  0.110 ( 0.109)\tLoss 1.7304e+00 (1.2674e+00)\tAcc@1  36.72 ( 53.31)\tAcc@5  82.03 ( 95.41)\n",
      "Test: [600/704]\tTime  0.110 ( 0.109)\tLoss 1.5224e+00 (1.2588e+00)\tAcc@1  54.69 ( 54.03)\tAcc@5  82.81 ( 95.03)\n",
      "Test: [650/704]\tTime  0.109 ( 0.109)\tLoss 1.1544e+00 (1.2455e+00)\tAcc@1  64.84 ( 54.93)\tAcc@5  88.28 ( 94.76)\n",
      "Test: [700/704]\tTime  0.110 ( 0.109)\tLoss 1.0837e+00 (1.2423e+00)\tAcc@1  58.59 ( 55.20)\tAcc@5  92.19 ( 94.43)\n",
      " * Acc@1 55.210 Acc@5 94.429\n",
      "lr: [0.0904508497187474]\n",
      "Epoch: [4][  0/704]\tTime  0.125 ( 0.125)\tData  0.090 ( 0.090)\tLoss 1.2860e+00 (1.2860e+00)\tAcc@1  57.03 ( 57.03)\tAcc@5  95.31 ( 95.31)\n",
      "Epoch: [4][ 50/704]\tTime  0.104 ( 0.104)\tData  0.086 ( 0.087)\tLoss 1.1765e+00 (1.1744e+00)\tAcc@1  53.12 ( 57.61)\tAcc@5  96.09 ( 95.47)\n",
      "Epoch: [4][100/704]\tTime  0.101 ( 0.103)\tData  0.083 ( 0.086)\tLoss 1.0728e+00 (1.1608e+00)\tAcc@1  61.72 ( 58.26)\tAcc@5  96.09 ( 95.45)\n",
      "Epoch: [4][150/704]\tTime  0.100 ( 0.103)\tData  0.083 ( 0.085)\tLoss 1.1059e+00 (1.1525e+00)\tAcc@1  57.03 ( 58.39)\tAcc@5  96.09 ( 95.52)\n",
      "Epoch: [4][200/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.085)\tLoss 1.1407e+00 (1.1586e+00)\tAcc@1  57.03 ( 58.11)\tAcc@5  94.53 ( 95.48)\n",
      "Epoch: [4][250/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.085)\tLoss 1.2227e+00 (1.1582e+00)\tAcc@1  57.03 ( 58.30)\tAcc@5  94.53 ( 95.48)\n",
      "Epoch: [4][300/704]\tTime  0.105 ( 0.102)\tData  0.086 ( 0.084)\tLoss 1.1903e+00 (1.1582e+00)\tAcc@1  56.25 ( 58.29)\tAcc@5  94.53 ( 95.52)\n",
      "Epoch: [4][350/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.3035e+00 (1.1567e+00)\tAcc@1  53.12 ( 58.37)\tAcc@5  94.53 ( 95.53)\n",
      "Epoch: [4][400/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.1974e+00 (1.1576e+00)\tAcc@1  58.59 ( 58.40)\tAcc@5  96.09 ( 95.48)\n",
      "Epoch: [4][450/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.1178e+00 (1.1569e+00)\tAcc@1  57.81 ( 58.46)\tAcc@5  97.66 ( 95.49)\n",
      "Epoch: [4][500/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.0364e+00 (1.1543e+00)\tAcc@1  60.94 ( 58.57)\tAcc@5  99.22 ( 95.52)\n",
      "Epoch: [4][550/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 9.7710e-01 (1.1530e+00)\tAcc@1  64.84 ( 58.66)\tAcc@5  99.22 ( 95.53)\n",
      "Epoch: [4][600/704]\tTime  0.102 ( 0.102)\tData  0.084 ( 0.084)\tLoss 1.2714e+00 (1.1543e+00)\tAcc@1  51.56 ( 58.59)\tAcc@5  95.31 ( 95.49)\n",
      "Epoch: [4][650/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.0436e+00 (1.1535e+00)\tAcc@1  62.50 ( 58.57)\tAcc@5  96.09 ( 95.50)\n",
      "Epoch: [4][700/704]\tTime  0.101 ( 0.102)\tData  0.083 ( 0.084)\tLoss 1.1410e+00 (1.1536e+00)\tAcc@1  60.16 ( 58.56)\tAcc@5  94.53 ( 95.49)\n",
      "Test: [  0/704]\tTime  0.093 ( 0.093)\tLoss 7.0308e-01 (7.0308e-01)\tAcc@1  77.34 ( 77.34)\tAcc@5  98.44 ( 98.44)\n",
      "Test: [ 50/704]\tTime  0.110 ( 0.103)\tLoss 6.0762e-01 (7.7345e-01)\tAcc@1  79.69 ( 72.50)\tAcc@5  96.88 ( 96.34)\n",
      "Test: [100/704]\tTime  0.110 ( 0.106)\tLoss 7.8522e-01 (8.7234e-01)\tAcc@1  68.75 ( 66.75)\tAcc@5  97.66 ( 96.50)\n",
      "Test: [150/704]\tTime  0.108 ( 0.107)\tLoss 1.4090e+00 (1.0047e+00)\tAcc@1  50.00 ( 61.64)\tAcc@5  93.75 ( 95.74)\n",
      "Test: [200/704]\tTime  0.110 ( 0.108)\tLoss 1.2271e+00 (1.1247e+00)\tAcc@1  54.69 ( 57.88)\tAcc@5  95.31 ( 95.61)\n",
      "Test: [250/704]\tTime  0.109 ( 0.108)\tLoss 1.6063e+00 (1.2403e+00)\tAcc@1  43.75 ( 54.06)\tAcc@5  93.75 ( 95.03)\n",
      "Test: [300/704]\tTime  0.110 ( 0.108)\tLoss 2.2424e+00 (1.3535e+00)\tAcc@1  17.19 ( 50.38)\tAcc@5  92.97 ( 94.92)\n",
      "Test: [350/704]\tTime  0.109 ( 0.109)\tLoss 1.9989e+00 (1.4329e+00)\tAcc@1  23.44 ( 47.88)\tAcc@5  98.44 ( 95.12)\n",
      "Test: [400/704]\tTime  0.110 ( 0.109)\tLoss 1.6715e+00 (1.4789e+00)\tAcc@1  35.94 ( 45.81)\tAcc@5  99.22 ( 95.09)\n",
      "Test: [450/704]\tTime  0.108 ( 0.109)\tLoss 5.5931e-01 (1.4660e+00)\tAcc@1  82.03 ( 46.62)\tAcc@5  96.88 ( 94.93)\n",
      "Test: [500/704]\tTime  0.110 ( 0.109)\tLoss 8.4486e-01 (1.3875e+00)\tAcc@1  71.09 ( 50.00)\tAcc@5  92.97 ( 94.92)\n",
      "Test: [550/704]\tTime  0.110 ( 0.109)\tLoss 1.6816e+00 (1.3679e+00)\tAcc@1  48.44 ( 51.40)\tAcc@5  75.78 ( 94.22)\n",
      "Test: [600/704]\tTime  0.110 ( 0.109)\tLoss 1.4856e+00 (1.3474e+00)\tAcc@1  55.47 ( 52.34)\tAcc@5  89.06 ( 94.16)\n",
      "Test: [650/704]\tTime  0.110 ( 0.109)\tLoss 1.1060e+00 (1.3242e+00)\tAcc@1  68.75 ( 53.34)\tAcc@5  89.06 ( 94.17)\n",
      "Test: [700/704]\tTime  0.110 ( 0.109)\tLoss 8.3952e-01 (1.3214e+00)\tAcc@1  70.31 ( 53.46)\tAcc@5  96.88 ( 93.86)\n",
      " * Acc@1 53.503 Acc@5 93.861\n",
      "lr: [0.08535533905932739]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "#    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    acc1 = validate(val_loader, model, criterion)\n",
    "\n",
    "    # remember best acc@1 and save checkpoint\n",
    "    is_best = acc1 > best_acc1\n",
    "    best_acc1 = max(acc1, best_acc1)\n",
    "\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': ARCH,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "    \n",
    "    scheduler.step()\n",
    "    print('lr: ' + str(scheduler.get_last_lr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c4cb64",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d6131",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "v3a-2",
   "language": "python",
   "display_name": "v3a-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}