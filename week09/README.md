# Week 9: HPC, MPI, and Multinode / MultiGPU (MNMG) Training 

History of HPC. HPC vs HTC/Big Data. Typical HPC problems. Architecture of supercomputers. 
Interconnect topologies: fat tree, torus. FLOPs, Top500. Amdahlâ€™s law. Programming for HPC systems. 
MPI. HPC schedulers. Infiniband vs TCP/IP. Google TPUs and TPU pods. Nvidia DGX systems and superpods. 
Magnum IO. Distributed Deep Learning Model training. 
Uber Horovod. Distributed Training in TensorFlow and PyTorch. Distributed Training in AWS and Azure. 

### Reading: 
* [Top 500 Supercomputers](https://www.top500.org/lists/top500/) (site review)
* [Infiniband in the top 500 supercomputers](https://blogs.nvidia.com/blog/2020/06/22/top500-isc-supercomputing/) (review)
* [Nvidia Selene blog](https://blogs.nvidia.com/blog/2020/08/14/making-selene-pandemic-ai/) (skim)
* [Training PyTorch models on Google TPU pods](https://cloud.google.com/tpu/docs/tutorials/pytorch-pod) (skim)
* [Distributed Data Parallel with PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) (review)
